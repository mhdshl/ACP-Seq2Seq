# -*- coding: utf-8 -*-
"""ACP_synthesis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mQi7eh-Z4l1g8faCakH2b50o4GFUik_F
"""

## import libraries
from __future__ import absolute_import, division, print_function, unicode_literals

# try:
#   # %tensorflow_version only exists in Colab.
#   %tensorflow_version 2.x
# except Exception:
#   pass

import os
import tensorflow as tf
import numpy as np
import pandas as pd
from keras.callbacks import EarlyStopping

# # list(acp240[0][0])
# def preprocess(acp240, acp740):
#   acp240_id = []
#   acp240_seq = []
#   for text in acp240[0]:
#     if ('>' in list(text)):
#       acp240_id.append(text)
#     else:
#       acp240_seq.append(text)

#   ## preprocess acp740
#   acp740_id = []
#   acp740_seq = []
#   for text in acp740[0]:
#     if ('>' in list(text)):
#       acp740_id.append(text)
#     else:
#       acp740_seq.append(text)

#   return acp240_id, acp240_seq, acp740_id, acp740_seq

def preprocess(dataset_path):
  acp_id = []
  acp_seq = []
  for text in dataset_path[0]:
    if ('>' in list(text)):
      acp_id.append(text)
    else:
      acp_seq.append(text)


  return acp_id, acp_seq

## seq_to_text
# def seq_to_text(acp240, acp740):
#   acp240_id, acp240_seq, acp740_id, acp740_seq = preprocess(acp240, acp740)
#   acp240_txt = '\n'.join(acp240_seq)
#   acp740_txt = '\n'.join(acp740_seq)
#   acp_txt = acp240_txt + '\n' + acp740_txt
#   return acp_txt
def seq_to_text(dataset_path):
  acp_id, acp_seq = preprocess(dataset_path)
  acp_txt = '\n'.join(acp240_seq)
  return acp_txt

## Unique characters in the sequeces
def unique_chars(acp_txt):

  aa_dict = sorted(set(acp_txt)) ### Sort index of iterables (enumerate(aa))

  ## Amino Acid to Index
  aa2idx = {u:i for i, u in enumerate(aa_dict)}

  ## Index to Amino Acid
  idx2aa = np.array(aa_dict)

  ## Create AA sequence as integer indices
  aa_as_int = np.array([aa2idx[u] for u in acp_txt]) # don't forget bracket '[]' to make it accessible.
  return aa2idx, idx2aa, aa_as_int, aa_dict

# def create_batch(acp240, acp740, seq_length = 64):
#   # seq_length = 64
#   acp_txt = seq_to_text(acp240, acp740)
#   aa2idx, idx2aa, aa_as_int, aa_dict = unique_chars(acp_txt)
#   examples_per_epoch = len(acp_txt)//(seq_length+1) ## '//' division & floor (내림)

#   ## Create tensorflow dataset
#   aa_dataset = tf.data.Dataset.from_tensor_slices(aa_as_int)

#   ## Create batch dataset using batch method in tf.Dataset class
#   seq_batch = aa_dataset.batch(seq_length+1, drop_remainder=True) ## drop the smaller batch
#   return seq_batch, aa_dataset, idx2aa, aa_dict
def create_batch(dataset_path, seq_length = 64):
  # seq_length = 64
  acp_txt = seq_to_text(dataset_path)
  aa2idx, idx2aa, aa_as_int, aa_dict = unique_chars(acp_txt)
  examples_per_epoch = len(acp_txt)//(seq_length+1) ## '//' division & floor (내림)

  ## Create tensorflow dataset
  aa_dataset = tf.data.Dataset.from_tensor_slices(aa_as_int)

  ## Create batch dataset using batch method in tf.Dataset class
  seq_batch = aa_dataset.batch(seq_length+1, drop_remainder=True) ## drop the smaller batch
  return seq_batch, aa_dataset, idx2aa, aa_dict

def split_input_target(chunk):
    input_seq = chunk[:-1]
    target_seq = chunk[1:]
    return input_seq, target_seq

# def create_dataset(acp240, acp740, BATCH_SIZE = 64, BUFFER_SIZE = 10000):
#   seq_batch, aa_dataset, idx2aa, aa_dict = create_batch(acp240, acp740)
#   dataset = seq_batch.map(split_input_target)
#   dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
#   return dataset

def create_dataset(dataset_path, BATCH_SIZE = 64, BUFFER_SIZE = 10000):
  seq_batch, aa_dataset, idx2aa, aa_dict = create_batch(dataset_path)
  dataset = seq_batch.map(split_input_target)
  dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
  return dataset

# def build_model(aa_dict, embedding_dim= 256, rnn_units= 1024, batch_size=64):
#   # seq_batch, aa_dataset, idx2aa, aa_dict = create_batch(acp240, acp740)
#   aa_dict_size= len(aa_dict)
#   model = tf.keras.Sequential([
#     tf.keras.layers.Embedding(aa_dict_size, embedding_dim, batch_input_shape = [batch_size, None]),
#     tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform', reset_after=True),
#     tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform', reset_after=True),
#     tf.keras.layers.Dense(aa_dict_size)
#   ])
#   return model
def build_model(aa_dict_size=21, embedding_dim= 256, rnn_units= 1024, batch_size=64):
  # seq_batch, aa_dataset, idx2aa, aa_dict = create_batch(acp240, acp740)
  # aa_dict_size= len(aa_dict)
  model = tf.keras.Sequential([
    tf.keras.layers.Embedding(aa_dict_size, embedding_dim, batch_input_shape = [batch_size, None]),
    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform', reset_after=True),
    tf.keras.layers.GRU(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform', reset_after=True),
    tf.keras.layers.Dense(aa_dict_size)
  ])
  return model

def generate_random_text(aa2idx, idx2aa, min_thres=11, max_thres=97, num_generate = 500):
  start_string=u"\n"
  ACP_ID = 1

  # acp_txt = seq_to_text(acp240, acp740)

  # aa2idx, idx2aa, aa_as_int, aa_dict = unique_chars(acp_txt)
  input_eval = [aa2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  for i in range(num_generate):
      predicted_id = np.random.randint(0,21)
      text_generated.append(idx2aa[predicted_id])
      if(text_generated[-1] == start_string):
        text_generated.append

  text_unfiltered = ''.join(text_generated)
  text_unfiltered = text_unfiltered.splitlines()

  # Empty string to store our results

  text_filtered = []
  for j in range(len(text_unfiltered)):
    if(len(text_unfiltered[j]) < min_thres):
      next
    elif(len(text_unfiltered[j]) > max_thres):
      next
    else:
      text_filtered.append('>ACP_'+str(ACP_ID)+'|'+'1' + start_string + ''.join(text_unfiltered[j])+'\n')
      ACP_ID += 1

  return (''.join(text_filtered))

def write_random_text(aa2idx, idx2aa, min_threshold, max_threshold, filename='random_from_func.txt', num_generate=5000):
  f = open(filename, 'w')
  f.write(generate_random_text(aa2idx, idx2aa, min_threshold, max_threshold, num_generate))
  f.close()

def loss(labels, logits):
  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

def train_gen_model(dataset, aa_dict, embedding_dim= 256, rnn_units= 1024, batch_size=64, EPOCHS=100):
  # seq_batch, aa_dataset, idx2aa, aa_dict = create_batch(acp240, acp740)
  # dataset = seq_batch.map(split_input_target)
  # dataset = create_dataset(acp240, acp740)
  model = build_model(aa_dict, embedding_dim, rnn_units, batch_size)
  model.compile(optimizer='adam', loss=loss)
  checkpoint_dir = './training_checkpoints'

  # Name of the checkpoint files
  checkpoint_prefix = os.path.join(checkpoint_dir, "model_epoch_{epoch}")
  es = EarlyStopping(monitor='loss', mode='min', verbose=0, patience=25)

  checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
      filepath=checkpoint_prefix,
      save_weights_only=True)

  # EPOCHS=100
  history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, es], verbose=2)
  # restore checkpoint
  tf.train.latest_checkpoint(checkpoint_dir)
  # load model
  model = build_model(aa_dict, embedding_dim, rnn_units, batch_size=1)
  model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
  model.build(tf.TensorShape([1, None]))
  return model

def generate_text(aa2idx, idx2aa, model, start_string=u"\n", min_thres=10, max_thres=250, num_generate = 5000, temperature = 1.0):

  ACP_ID = 1
  # acp_txt = seq_to_text(acp240, acp740)

  # aa2idx, idx2aa, aa_as_int, aa_dict = unique_chars(acp_txt)

  # Converting our start string to numbers (vectorizing)
  input_eval = [aa2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []


  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the word returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted word as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2aa[predicted_id])
      if(text_generated[-1] == start_string):
        text_generated.append

  text_unfiltered = ''.join(text_generated)
  text_unfiltered = text_unfiltered.splitlines()

  # Empty string to store our results
  text_filtered = []
  for j in range(len(text_unfiltered)):
    if(len(text_unfiltered[j]) < min_thres):
      next
    elif(len(text_unfiltered[j]) > max_thres):
      next
    else:
      text_filtered.append('>ACP_'+str(ACP_ID)+'|'+'1' + start_string + ''.join(text_unfiltered[j])+'\n')
      ACP_ID += 1

  return (''.join(text_filtered))

def write_file(aa2idx,idx2aa,model,min_thres=11,max_thres=97,filename='gen_from_model.txt',start_string=u"\n",num_generate=5000,temperature=1.0):
  # Evaluation step (generating text using the learned model
  f = open(filename, 'w')
  f.write(generate_text(aa2idx,idx2aa, model,min_thres,max_thres, start_string, num_generate, temperature))
  f.close()

def train_and_generate_text(acp240, acp740, start_string, min_thres=10, max_thres=250, num_generate = 5000, temperature = 1.0):
  # Evaluation step (generating text using the learned model)
  model = train_gen_model(acp240, acp740, EPOCHS=100)
  # Number of characters to generate
  # num_generate = 5000
  ACP_ID = 1
  acp_txt = seq_to_text(acp240, acp740)

  aa2idx, idx2aa, aa_as_int, aa_dict = unique_chars(acp_txt)

  # Converting our start string to numbers (vectorizing)
  input_eval = [aa2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  # temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the word returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted word as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2aa[predicted_id])
      if(text_generated[-1] == start_string):
        text_generated.append

  text_unfiltered = ''.join(text_generated)
  text_unfiltered = text_unfiltered.splitlines()

  # Empty string to store our results
  text_filtered = []
  for j in range(len(text_unfiltered)):
    if(len(text_unfiltered[j]) < min_thres):
      next
    elif(len(text_unfiltered[j]) > max_thres):
      next
    else:
      text_filtered.append('>ACP_'+str(ACP_ID)+'|'+'1' + start_string + ''.join(text_unfiltered[j])+'\n')
      ACP_ID += 1

  return (''.join(text_filtered))

def train_and_write_file(acp240, acp740, filename='combined_trained_seq.txt', start_string=u"\n", num_generate = 5000):
  f = open(filename, 'w')
  f.write(train_and_generate_text(acp240, acp740, start_string=u"\n", num_generate = 5000))
  f.close()
